---
title: 重新理解简单回归相关的一些概念
date: 2021-07-10 22:46:37
categories: 机器学习
tags: 

---

![cover](/img/regression-concept.assets/cover.jpg.webp.webp)



这两天准备手写一个手写数字识别, 但是在推导公式的时候感觉还是有点费劲, 可能是对之前的那些概念理解不是很深. 先去研究一下一些概念的本质含义吧.

<!--more-->

### Loss函数?

首先我遇到的直接问题就是怎么写多分类的loss, 研究一下Logistic二分类中是使用的交叉熵来定义的Loss. 这个可能可以从两个方向去理解. 第一个方向是交叉熵是什么, 第二个方向就像我之前文章里说的那样, 交叉熵是二分类问题中的似然函数.

### 相对熵(KL散度)

相对熵或KL散度是衡量两个分布差异的一个量. 

相对熵的定义是, 站在A的角度看, B与A概率对数差的期望.

用数学公式表达是:
$$
D_{KL}(A||B) = -\sum_i P(A_i) \ln\frac{P(B_i)}{P(A_i)}
$$
站在A的角度看的意思是, 某个概率对数差发生的概率是对应A取该值的概率.

重要的是, 相对熵并不满足对称性, 也就是说站在B角度看和站在A角度看得到的结果是不一样的.

站在B角度看的相对熵:
$$
D_{KL}(B||A) = -\sum P(B_i) \ln\frac{P(A_i)}{P(B_i)}
$$
也就是说, $D_{KL}(A||B) \neq D_{KL}(B||A)$.



### 交叉熵

把上面A角度下的相对熵公式进行变形:
$$
D_{KL}(A||B) = \sum_i P(A_i)\ln P(A_i) - \sum_iP(A_i)\ln P(B_i)
$$
可以惊奇地发现, 第一项就是A的熵, 如果A的分布不变, 那他就是个常数.

我们把后一项拿下来, 当作A对B的交叉熵.

注意一下, 自己最自己的交叉熵其实就是自己的熵, 并不一定是0.



### 分布

这个概念也挺重要, 虽然很基本.

高中学的分布列其实就是把一个随机变量所有可能取值对应的概率列出来. 

一个随机变量在每一个值上不同的概率分布情况.



在训练的过程中, 其实出现了两个分布. 一个是正确的分布, 就是训练集的标签, 在二分类问题中, 这个是一个在一类的概率为1, 另一类概率为0的一个分布, 随机变量两种取值(在第一类和在第二类).

在多分类问题中, 这个实际上是一个在一个类的概率为1, 其他类的概率都为0的一个分布.

另一个分布是模型预测出的分布, 他对于输入, 给出这个输入在每一类他认为的概率. 

**我们的目标就是让这两个分布(预测出的分布和实际分布)差异尽量小.**



实际分布是不变的, 所以我们取**实际分布对预测分布的**交叉熵. 为了让这两个分布差异尽量小, 我们需要让交叉熵尽量小. 所以我们用交叉熵当作Loss函数, 然后最小化Loss函数. 这就是第一种理解方法.



第二种理解方法是纯数学上的.

### 似然

似然函数也是一种条件概率, 但是他考虑的因果关系是相反的.

正常我们考虑一个分类问题, 是求在给定的参数下, 这个输入属于哪一类的分布.

但是似然函数研究的是, 现在知道这个输入属于哪一类, 要求这个参数取值的相关信息. 

这就很有贝叶斯公式的味道. 但是似然函数研究的并不是概率.



### 似然性/极大似然估计

似然函数终究还没有考虑现在的自变量是参数, 他写出的还是这个事件发生的概率和参数的关系, 我们只是人为地把自变量换成了参数. 所以似然函数得到的并不是"参数取这个值的可能性", 而"参数取这个值的时候, 这个事件发生的可能性".

这个和概率是很不一样的. 首先, 在给定参数下, 对于一个随机变量X, 他所有取值的概率加起来一定是1. 但是对于参数的每个取值, 这些似然函数得到的值加起来并不一定为1. 我们把这个似然函数得到的值叫做似然性.

直接看似然性的取值是没有任何意义的, 只有两个同一问题下的似然性在一起比较的时候, 似然性更大的那个参数取值才被我们认为是参数更有可能取到的值. 所以, 我们可以找到似然性最大的参数取值, 并认为他就是参数的最可能取值, 这就叫极大似然估计.



实际上, 交叉熵就是多分类问题的似然函数加个负号., 在之前说Logistic回归的时候, 已经证明了在二分类问题中的交叉熵就是似然函数加负号(好像只是放个了草稿). 为了做极大似然估计, 我们需要最小化交叉熵, 这是另一种理解交叉熵的方法.



本来准备11号把这个文章理完的, 但是11号出去玩了一天, 上午去momopark中午去赛格下午去宜家, 宜家真是好地方, 把里面最贵的沙发和床都体验了一遍. 就是一号线好像都不是很新的样子, 从皂河地铁站出来每上一个台阶感觉周围的气温都上升了1度, 一出门看一群三轮车喊宜家宜家我才确认自己来对了地方. 饭也有点贵, 随便吃个牛排啥的就四五十了, 不过土豆泥还是一如既往的吃不完, 我就从来没吃完过宜家的土豆泥. 

还有饮料就离谱, 看出了个新口味, 什么越橘, 第一次去接了一杯是透明的, 我以为就这么高级, 结果发现这长的这么像水的东西喝起来居然也跟水一模一样. 过一会再去接一杯才知道这玩意应该是粉红色的酸不拉几的, 我合理推测我第一次接的时候是刚加了原料之类的, 还没搞好就被我接了. 临走的时候99说想喝无糖可乐, 然后我去接了一杯, 接出来了纯水密堆积无糖可乐填空隙的淡棕色的东西, 喝起来跟丁香食堂某些窗口刚开业的时候送的兑水可乐一个味, 接了几杯颜色就没变过, 就放弃了. 餐盘都送了才看到有人接出来了最起码像无糖可乐的东西, 血亏.

这两天还在尝试自己手推多分类问题的过程, 明天上午送99去北站, 下午去师大附中讲课, 争取地铁上推完晚上就写.



